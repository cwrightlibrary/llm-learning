{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5c1e61",
   "metadata": {},
   "source": [
    "# LLM Learning\n",
    "\n",
    "Following [this guide](https://www.youtube.com/watch?v=UU1WVnMk4E8) to learn about large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47bdb8f",
   "metadata": {},
   "source": [
    "## Setting up with a small training model using [The Lord of the Rings](lotr.txt) text\n",
    "\n",
    "First, we open the file and read it with `text` and then we get each character (sorted) from that into `chars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9109e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc4a9ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "with open(\"lotr.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text)) # gets all of the characters in the text file using set, then sorts them\n",
    "vocabulary_size = len(chars)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30d8d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i, ch in enumerate(chars) }      # creates a dict \"\\n\": 0, \" \": 1, etc.\n",
    "int_to_string = { i:ch for i, ch in enumerate(chars) }      # creates a dict 0: \"\\n\", 1: \" \", etc.\n",
    "encode = lambda s: [string_to_int[c] for c in s]            # converts an input string to a list of ints using the string_to_int map\n",
    "decode = lambda l: \"\".join([int_to_string[i] for i in l])   # converts an input list of ints to a string using the int_to_string map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e175ad",
   "metadata": {},
   "source": [
    "To show how these functions work, let's look at them in a more traditional way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f6eff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def string_to_int():\n",
    "#     return_dict = {}\n",
    "#     for i, ch in enumerate(chars):\n",
    "#         return_dict[ch] = i\n",
    "#     return return_dict\n",
    "\n",
    "# def int_to_string():\n",
    "#     return_dict = {}\n",
    "#     for i, ch in enumerate(chars):\n",
    "#         return_dict[str(i)] = ch\n",
    "#     return return_dict\n",
    "\n",
    "# def encode(s):\n",
    "#     return_s = []\n",
    "#     for c in s:\n",
    "#         return_s.append(string_to_int[c])\n",
    "#     return return_s\n",
    "\n",
    "# def decode(s):\n",
    "#     return_s = \"\"\n",
    "#     for i in s:\n",
    "#         return_s += int_to_string[i]\n",
    "#     return return_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97580dc5",
   "metadata": {},
   "source": [
    "To see how `encode` and `decode` works, here's an example of encoding and decoding *\"The Lord of the Rings\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24837a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 60, 57, 1, 36, 67, 70, 56, 1, 67, 58, 1, 72, 60, 57, 1, 42, 61, 66, 59, 71]\n",
      "The Lord of the Rings\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"The Lord of the Rings\"))\n",
    "print(decode(encode(\"The Lord of the Rings\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771fa5e1",
   "metadata": {},
   "source": [
    "### Some info on tokenizers\n",
    "\n",
    "The example above, `string_to_int`, `int_to_string`, `encode`, and `decode`, uses **character-level tokenizers**. This takes each character and converts it to its equivalent integer and back. This leaves us with a small vocabulary and a large amount of tokens to convert.\n",
    "\n",
    "### On `pytorch`\n",
    "\n",
    "Having simple lists for the `encoded` data isn't efficient, so we'll use `pytorch`'s tensors instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dfcd771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([27, 60, 53, 68, 72, 57, 70,  1, 33,  0, 25, 66,  1, 45, 66, 57, 76, 68,\n",
      "        57, 55, 72, 57, 56,  1, 40, 53, 70, 72, 77,  0, 33, 66,  1, 53,  1, 60,\n",
      "        67, 64, 57,  1, 61, 66,  1, 72, 60, 57,  1, 59, 70, 67, 73, 66, 56,  1,\n",
      "        72, 60, 57, 70, 57,  1, 64, 61, 74, 57, 56,  1, 53,  1, 60, 67, 54, 54,\n",
      "        61, 72, 10,  1, 38, 67, 72,  1, 53,  1, 66, 53, 71, 72, 77,  8,  1, 56,\n",
      "        61, 70, 72, 77,  8,  1, 75, 57, 72,  1])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)       # encodes as tensor with the datatype as a long list of integers\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6385afb1",
   "metadata": {},
   "source": [
    "### Some info on training and validation splits\n",
    "\n",
    "Let's say we have a text corpus, a document with tons of text. We would make our training set 80% of it and validation would be the remaining 20%. If we, instead, trained on the entire 100%, it would essentially memorize the entire piece of text and we wouldn't get anything useful out of it. The purpose of language modeling is to generate text that's like the training data, which is why we put it into splits. So, when we use the training split, it'll memorize the 80% and will only generate on that 80%. We do this to make sure that the generations are unique.\n",
    "\n",
    "### On the name of this file, `bigram` language\n",
    "\n",
    "The definition for bigram is: *a pair of consecutive written units such as letters, syllables, or words*. Let's say we have the word \"Hello\".\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| Start of context | \"H\" |\n",
    "| \"H\" | \"e\" |\n",
    "| \"l\" | \"l\" |\n",
    "| \"l\" | \"o\" |\n",
    "\n",
    "### On inputs and targets\n",
    "\n",
    "It only uses the previous character to predict the next. Looking further into it, consider this info:\n",
    "\n",
    "```python\n",
    "block_size = 5\n",
    "\n",
    "# ... [5, 67, 21, 58, 40] 35 ...    [:block_size]\n",
    "# ... 5 [67, 21, 58, 40, 35] ...    [1:block_size+1]\n",
    "```\n",
    "\n",
    "In the above example, we use one list of integers, the input, with another beneath, which are the same as the first but offset by one, which are the targets for prediction. The `block_size` is the amount of training characters for predictions. We then look at the difference the target is away from the prediction to train the model.\n",
    "\n",
    "Below, let's implement it in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e3278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([27, 60, 53,  ...,  1, 67, 66])\n",
      "when input is tensor([27]) target is 60\n",
      "when input is tensor([27, 60]) target is 53\n",
      "when input is tensor([27, 60, 53]) target is 68\n",
      "when input is tensor([27, 60, 53, 68]) target is 72\n",
      "when input is tensor([27, 60, 53, 68, 72]) target is 57\n",
      "when input is tensor([27, 60, 53, 68, 72, 57]) target is 70\n",
      "when input is tensor([27, 60, 53, 68, 72, 57, 70]) target is 1\n",
      "when input is tensor([27, 60, 53, 68, 72, 57, 70,  1]) target is 33\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8 * len(data))    # the split point in the data (80%)\n",
    "train_data = data[:n]       # the training data is the first 80% of the data\n",
    "val_data = data[n:]         # the validation data is the last 20% of the data\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "print(train_data)\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd027aa",
   "metadata": {},
   "source": [
    "While this can do predictions, it is not scalable. This is sequential, which is what a CPU does, which can operate quickly but only sequentially. When using GPUs, we can do simpler tasks very quickly or in parallel. So, if using a GPU, we would take a bunch of these operations and stack them for a GPU to run at the same time, which will scale or training data. So, using the code from above, we could get multiple `x`, `y` variables and do the training `for` loop at once, instead of one at a time.\n",
    "\n",
    "This batch of blocks is a hyperparameter called `batch_size`. So, we would have maybe 8 `batches` of `block`s that are 12 in size. In other words, the `block_size` is the length of the sequence and the `batch_size` is how many of these are running at the same time. Without using a GPU, we wouldn't get the speed or performance we'd get with one. Since I don't have a GPU and will be using my M1 Mac, I won't get the performance I'd get with a GPU but the M1 chip will perform very well still for my uses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d72cbf",
   "metadata": {},
   "source": [
    "### `torch` examples and info\n",
    "\n",
    "Check out the [torch-examples](torch-examples.ipynb) file to see some of `torch`'s capabilities and some m1 (GPU)/CPU comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a45f92",
   "metadata": {},
   "source": [
    "Continue with [More `PyTorch` Functions](https://www.youtube.com/watch?v=UU1WVnMk4E8&t=2869s&pp=0gcJCdkCDuyUWbzu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
